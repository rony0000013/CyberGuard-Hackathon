{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, wandb\n",
    "import torch\n",
    "import spacy\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.read_csv(\"train.csv\")\n",
    "test = pl.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.filter(pl.col(\"crimeaditionalinfo\").is_not_null())\n",
    "test = test.filter(pl.col(\"crimeaditionalinfo\").is_not_null())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/miniforge3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "spacy.prefer_gpu()\n",
    "spacy_nlp = spacy.load(\"en_core_web_md\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97b6add58214e00a4f9a4d96f9c49c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfaa29f7bc394bfcaa6caa20b5df0ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_text = []\n",
    "for text in tqdm(train[\"crimeaditionalinfo\"]):\n",
    "    train_text.append(\" \".join([d.lemma_ for d in spacy_nlp(text) if not d.is_punct]))\n",
    "test_text = []\n",
    "for text in tqdm(test[\"crimeaditionalinfo\"]):\n",
    "    test_text.append(\" \".join([d.lemma_ for d in spacy_nlp(text) if not d.is_punct]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.with_columns(pl.col(\"category\").str.to_lowercase(), \n",
    "                    pl.col(\"sub_category\").str.to_lowercase().fill_null(\"NULL\"), \n",
    "                    pl.col(\"crimeaditionalinfo\").str.to_lowercase().str.strip_chars())\n",
    "test = test.with_columns(pl.col(\"category\").str.to_lowercase(), \n",
    "                    pl.col(\"sub_category\").str.to_lowercase().fill_null(\"NULL\"), \n",
    "                    pl.col(\"crimeaditionalinfo\").str.to_lowercase().str.strip_chars())\n",
    "\n",
    "train = train.with_columns(pl.Series(\"text\", train_text).str.to_lowercase().str.strip_chars())\n",
    "test = test.with_columns(pl.Series(\"text\", test_text).str.to_lowercase().str.strip_chars())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Null text use category = \"online financial fraud\",  sub_category = \"upi related frauds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class = train[\"category\"] + \" - \" + train[\"sub_category\"]\n",
    "test_class = test[\"category\"] + \" - \" + test[\"sub_category\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "x = le.fit_transform(train_class)\n",
    "train = train.with_columns(pl.Series(\"label\", x))\n",
    "le.classes_ = np.append(le.classes_, \"NULL\")\n",
    "\n",
    "x = le.transform([\"NULL\" if x not in le.classes_ else x for x in test_class])\n",
    "test = test.with_columns(pl.Series(\"label\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.write_csv(\"train_cleaned.csv\")\n",
    "test.write_csv(\"test_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.read_csv(\"train_cleaned.csv\")\n",
    "test = pl.read_csv(\"test_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "train_tf = tf_idf.fit_transform(train[\"text\"])\n",
    "test_tf = tf_idf.transform(test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8591bf7719e4a4c8e1bf66aa5ed0093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c430cff7b1d348f8b38324f4d02a1835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "train_emb = model.encode(train[\"text\"].to_list(), batch_size=128, show_progress_bar=True, device=\"cuda\")\n",
    "test_emb = model.encode(test[\"text\"].to_list(), batch_size=128, show_progress_bar=True, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb = np.load(\"train_emb.npy\")\n",
    "test_emb = np.load(\"test_emb.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 52.15%\n",
      "Epoch [2/10], Test Accuracy: 53.43%\n",
      "Epoch [3/10], Test Accuracy: 53.85%\n",
      "Epoch [4/10], Test Accuracy: 54.21%\n",
      "Epoch [5/10], Test Accuracy: 54.30%\n",
      "Epoch [6/10], Test Accuracy: 54.62%\n",
      "Epoch [7/10], Test Accuracy: 54.32%\n",
      "Epoch [8/10], Test Accuracy: 54.28%\n",
      "Epoch [9/10], Test Accuracy: 54.85%\n",
      "Epoch [10/10], Test Accuracy: 54.71%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert the model architecture\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.output = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return self.output(x)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.FloatTensor(train_emb)\n",
    "y_train = torch.LongTensor(train[\"label\"].to_numpy())\n",
    "X_test = torch.FloatTensor(test_emb)\n",
    "y_test = torch.LongTensor(test[\"label\"].to_numpy())\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Initialize model, loss function and optimizer\n",
    "model = TextClassifier(train_emb.shape[1], train[\"label\"].n_unique())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 54.71%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return accuracy, all_predictions, all_labels\n",
    "\n",
    "# Run evaluation\n",
    "model.eval()\n",
    "accuracy, predictions, true_labels = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159707 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 97920\n",
      "[LightGBM] [Info] Number of data points in the train set: 93665, number of used features: 384\n",
      "[LightGBM] [Info] Start training from score -2.153074\n",
      "[LightGBM] [Info] Start training from score -5.509944\n",
      "[LightGBM] [Info] Start training from score -5.273694\n",
      "[LightGBM] [Info] Start training from score -5.265395\n",
      "[LightGBM] [Info] Start training from score -5.224904\n",
      "[LightGBM] [Info] Start training from score -5.155911\n",
      "[LightGBM] [Info] Start training from score -5.191730\n",
      "[LightGBM] [Info] Start training from score -5.167084\n",
      "[LightGBM] [Info] Start training from score -5.216998\n",
      "[LightGBM] [Info] Start training from score -5.199437\n",
      "[LightGBM] [Info] Start training from score -6.366076\n",
      "[LightGBM] [Info] Start training from score -6.765349\n",
      "[LightGBM] [Info] Start training from score -5.592408\n",
      "[LightGBM] [Info] Start training from score -7.535457\n",
      "[LightGBM] [Info] Start training from score -4.431767\n",
      "[LightGBM] [Info] Start training from score -6.958843\n",
      "[LightGBM] [Info] Start training from score -3.853099\n",
      "[LightGBM] [Info] Start training from score -3.131424\n",
      "[LightGBM] [Info] Start training from score -6.391234\n",
      "[LightGBM] [Info] Start training from score -3.707250\n",
      "[LightGBM] [Info] Start training from score -7.663290\n",
      "[LightGBM] [Info] Start training from score -8.080184\n",
      "[LightGBM] [Info] Start training from score -4.631840\n",
      "[LightGBM] [Info] Start training from score -6.564678\n",
      "[LightGBM] [Info] Start training from score -3.811210\n",
      "[LightGBM] [Info] Start training from score -5.414394\n",
      "[LightGBM] [Info] Start training from score -6.237994\n",
      "[LightGBM] [Info] Start training from score -5.777599\n",
      "[LightGBM] [Info] Start training from score -2.159993\n",
      "[LightGBM] [Info] Start training from score -4.812847\n",
      "[LightGBM] [Info] Start training from score -3.141749\n",
      "[LightGBM] [Info] Start training from score -2.781522\n",
      "[LightGBM] [Info] Start training from score -2.356937\n",
      "[LightGBM] [Info] Start training from score -1.249720\n",
      "[LightGBM] [Info] Start training from score -5.351655\n",
      "[LightGBM] [Info] Start training from score -7.422128\n",
      "[LightGBM] [Info] Start training from score -3.502279\n",
      "[LightGBM] [Info] Start training from score -11.447480\n",
      "[LightGBM] [Info] Start training from score -4.100180\n",
      "[LightGBM] [Info] Start training from score -3.931047\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3208955223880597,\n",
       " array([[687,  31,  38, ...,  35,  29,   0],\n",
       "        [  9,   9,   1, ...,   1,   6,   0],\n",
       "        [ 16,   2,  30, ...,   2,   0,   0],\n",
       "        ...,\n",
       "        [ 45,  18,   3, ...,  14,  19,   0],\n",
       "        [ 54,  11,   2, ...,   8,  29,   0],\n",
       "        [  0,   0,   0, ...,   0,   0,   0]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_classifier = LGBMClassifier()\n",
    "naive_bayes_classifier.fit(train_emb, train[\"label\"])\n",
    "\n",
    "#predicted y\n",
    "y_pred = naive_bayes_classifier.predict(test_emb)\n",
    "accuracy_score(test[\"label\"], y_pred), confusion_matrix(test[\"label\"], y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
